services:
  jupyter:
    image: franzdiebold/datascience-ultimate:latest
    ports:
      - 8888:8888
    volumes:
      - ./jupyter:/usr/src/app
    working_dir: /usr/src/app
    models:
      - smollm2
      - gemma3n
    depends_on:
      - ollama
      - open-webui

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ./ollama/ollama:/root/.ollama
      - ./ollama/entrypoint.sh:/entrypoint.sh
      - ./ollama/models_to_preload.txt:/models_to_preload.txt
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    volumes:
      - ./open_webui:/app/backend/data
    models:
      - smollm2
      - gemma3n
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - OLLAMA_BASE_URLS=http://ollama:11434
      - ENV=dev
      - WEBUI_AUTH=False
      - OPENAI_API_BASE_URL=http://model-runner.docker.internal/engines/v1
      - OPENAI_API_KEY=na

  langgraph-server:
    build: ./langgraph_server
    volumes:
      - ./langgraph_server:/app
    ports:
      - 2024:2024
    models:
      - smollm2
      - gemma3n
    depends_on:
      - ollama

  agent-inbox:
    image: node:latest
    volumes:
      - ./agent_inbox:/usr/src/app
    working_dir: /usr/src/app
    ports:
      - "3000:3000"
    entrypoint: ["/bin/bash", "entrypoint.sh"]

  agent-chat-ui:
    image: node:latest
    volumes:
      - ./agent_chat_ui:/usr/src/app
    working_dir: /usr/src/app
    ports:
      - "5173:5173"
    entrypoint: ["/bin/bash", "entrypoint.sh"]

  mcp-inspector:
    image: ghcr.io/modelcontextprotocol/inspector:latest
    ports:
      - "6274:6274"
      - "6277:6277"
    environment:
      - ALLOWED_ORIGINS=http://0.0.0.0:6274,http://localhost:6274
      - HOST=0.0.0.0
      - DANGEROUSLY_OMIT_AUTH=true

models:
  smollm2:
    model: ai/smollm2
  gemma3n:
    model: ai/gemma3n
